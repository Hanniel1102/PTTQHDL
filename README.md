# ğŸ¦ Credit Default Risk Prediction - Dá»± Ä‘oÃ¡n Kháº£ nÄƒng Vá»¡ Ná»£

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-1.0+-orange.svg)](https://scikit-learn.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

Dá»± Ã¡n phÃ¢n tÃ­ch vÃ  xÃ¢y dá»±ng mÃ´ hÃ¬nh Machine Learning Ä‘á»ƒ dá»± Ä‘oÃ¡n kháº£ nÄƒng vá»¡ ná»£ cá»§a khÃ¡ch hÃ ng vay tÃ­n dá»¥ng, sá»­ dá»¥ng dataset Home Credit Default Risk.

## ğŸ“‹ Má»¥c lá»¥c

- [Tá»•ng quan](#-tá»•ng-quan)
- [Dataset](#-dataset)
- [Quy trÃ¬nh phÃ¢n tÃ­ch](#-quy-trÃ¬nh-phÃ¢n-tÃ­ch)
- [Ká»¹ thuáº­t sá»­ dá»¥ng](#-ká»¹-thuáº­t-sá»­-dá»¥ng)
- [Káº¿t quáº£](#-káº¿t-quáº£)
- [CÃ i Ä‘áº·t](#-cÃ i-Ä‘áº·t)
- [Sá»­ dá»¥ng](#-sá»­-dá»¥ng)
- [Cáº¥u trÃºc thÆ° má»¥c](#-cáº¥u-trÃºc-thÆ°-má»¥c)
- [ÄÃ³ng gÃ³p](#-Ä‘Ã³ng-gÃ³p)
- [License](#-license)

## ğŸ¯ Tá»•ng quan

### Má»¥c tiÃªu
XÃ¢y dá»±ng mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n kháº£ nÄƒng vá»¡ ná»£ (default risk) cá»§a khÃ¡ch hÃ ng dá»±a trÃªn:
- ThÃ´ng tin nhÃ¢n kháº©u há»c
- Lá»‹ch sá»­ tÃ i chÃ­nh
- ThÃ´ng tin khoáº£n vay
- Äiá»ƒm tÃ­n dá»¥ng bÃªn ngoÃ i

### Váº¥n Ä‘á» kinh doanh
Trong ngÃ nh cho vay tÃ­n dá»¥ng, viá»‡c dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c khÃ¡ch hÃ ng cÃ³ kháº£ nÄƒng vá»¡ ná»£ giÃºp:
- âœ… Giáº£m thiá»ƒu rá»§i ro tÃ i chÃ­nh
- âœ… Tá»‘i Æ°u hÃ³a quyáº¿t Ä‘á»‹nh phÃª duyá»‡t khoáº£n vay
- âœ… TÄƒng lá»£i nhuáº­n báº±ng cÃ¡ch cho vay Ä‘Ãºng Ä‘á»‘i tÆ°á»£ng
- âœ… Giáº£m chi phÃ­ xá»­ lÃ½ ná»£ xáº¥u

### Äá»™ khÃ³
- **Dá»¯ liá»‡u máº¥t cÃ¢n báº±ng nghiÃªm trá»ng**: Tá»· lá»‡ vá»¡ ná»£ chá»‰ ~8% (1:11.5)
- **Nhiá»u missing values**: Má»™t sá»‘ cá»™t thiáº¿u >70% dá»¯ liá»‡u
- **Outliers nhiá»u**: Do tÃ­nh cháº¥t nghiá»‡p vá»¥ (khÃ¡ch hÃ ng cao cáº¥p, khoáº£n vay lá»›n)
- **High-dimensional**: 100+ features ban Ä‘áº§u

## ğŸ“Š Dataset

### Nguá»“n dá»¯ liá»‡u
- **TÃªn**: Home Credit Default Risk
- **Nguá»“n**: [Kaggle Competition](https://www.kaggle.com/c/home-credit-default-risk)
- **KÃ­ch thÆ°á»›c**: 307,511 samples Ã— 122 features

### Biáº¿n má»¥c tiÃªu (TARGET)
- `0`: KhÃ´ng vá»¡ ná»£ (91.9% - 282,686 samples)
- `1`: Vá»¡ ná»£ (8.1% - 24,825 samples)

### CÃ¡c nhÃ³m features chÃ­nh
1. **ThÃ´ng tin cÃ¡ nhÃ¢n**: Tuá»•i, giá»›i tÃ­nh, há»c váº¥n, tÃ¬nh tráº¡ng hÃ´n nhÃ¢n
2. **ThÃ´ng tin tÃ i chÃ­nh**: Thu nháº­p, giÃ¡ trá»‹ tÃ i sáº£n, sá»‘ tiá»n vay
3. **ThÃ´ng tin khoáº£n vay**: Loáº¡i khoáº£n vay, má»¥c Ä‘Ã­ch, thá»i háº¡n
4. **Äiá»ƒm tÃ­n dá»¥ng**: EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3
5. **ThÃ´ng tin viá»‡c lÃ m**: Nghá» nghiá»‡p, thÃ¢m niÃªn cÃ´ng viá»‡c

## ğŸ”„ Quy trÃ¬nh phÃ¢n tÃ­ch

### PHáº¦N 1: Exploratory Data Analysis (EDA)

Pipeline EDA Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ **KHÃ”NG THAY Äá»”I** dá»¯ liá»‡u gá»‘c, chá»‰ phÃ¢n tÃ­ch vÃ  hiá»ƒu dá»¯ liá»‡u.

#### BÆ°á»›c 1: ThÃ´ng tin cÆ¡ báº£n
- KÃ­ch thÆ°á»›c dataset
- Kiá»ƒu dá»¯ liá»‡u cÃ¡c cá»™t
- Bá»™ nhá»› sá»­ dá»¥ng

#### BÆ°á»›c 2: PhÃ¢n tÃ­ch Missing Values
```
Tá»•ng quan:
â”œâ”€â”€ 67 cá»™t cÃ³ missing values
â”œâ”€â”€ 41 cá»™t cÃ³ >30% missing
â”œâ”€â”€ 16 cá»™t cÃ³ >70% missing (Ä‘Ã¡nh dáº¥u Ä‘á»ƒ xÃ³a)
â””â”€â”€ Chiáº¿n lÆ°á»£c: XÃ³a >70%, impute <70%
```

#### BÆ°á»›c 3: Kiá»ƒm tra cháº¥t lÆ°á»£ng dá»¯ liá»‡u
- âœ… KhÃ´ng cÃ³ duplicates (SK_ID_CURR)
- âš ï¸ Imbalanced data: 91.9% vs 8.1%
- âš ï¸ Logic errors:
  - Unemployed nhÆ°ng cÃ³ DAYS_EMPLOYED
  - CNT_CHILDREN Ã¢m
  - AMT_INCOME_TOTAL â‰¤ 0

#### BÆ°á»›c 4: PhÃ¢n tÃ­ch phÃ¢n bá»‘ vÃ  Outliers
- Skewness analysis: 43 cá»™t cÃ³ |skew| > 1
- Outliers detection (IQR method)
- âš ï¸ **Quan trá»ng**: KHÃ”NG xÃ³a outliers trong credit scoring

#### BÆ°á»›c 5: PhÃ¢n tÃ­ch tÆ°Æ¡ng quan
- Ma tráº­n tÆ°Æ¡ng quan cho biáº¿n tÃ i chÃ­nh
- TÆ°Æ¡ng quan vá»›i TARGET
- PhÃ¡t hiá»‡n multicollinearity (|r| > 0.7)

#### BÆ°á»›c 6-7: PhÃ¢n tÃ­ch biáº¿n phÃ¢n loáº¡i
- PhÃ¢n bá»‘ TARGET theo giá»›i tÃ­nh, há»c váº¥n, loáº¡i thu nháº­p
- Nháº­n diá»‡n cÃ¡c nhÃ³m rá»§i ro cao

### PHáº¦N 2: Data Processing & Modeling

#### BÆ°á»›c 1-2: LÃ m sáº¡ch cÆ¡ báº£n
```python
âœ“ XÃ³a duplicates
âœ“ Sá»­a logic errors
âœ“ XÃ³a cá»™t cÃ³ >70% missing
âœ“ Feature Engineering (10+ features má»›i)
```

#### BÆ°á»›c 3-5: Tiá»n xá»­ lÃ½
```python
âœ“ Impute missing values (median/mode)
âœ“ Log-transform cho biáº¿n lá»‡ch (skew > 1)
âœ“ One-hot encoding cho categorical variables
âœ“ Loáº¡i bá» features tÆ°Æ¡ng quan cao (>0.95)
```

#### BÆ°á»›c 6-7: Chuáº©n bá»‹ dá»¯ liá»‡u
```python
âœ“ Train-test split (80/20, stratified)
âœ“ Feature Selection (Mutual Information)
âœ“ Giáº£m tá»« 200+ â†’ 160 features quan trá»ng
```

#### BÆ°á»›c 8-10: Tá»‘i Æ°u hÃ³a nÃ¢ng cao
```python
âœ“ RobustScaler normalization (tá»‘t cho outliers)
âœ“ SMOTETomek resampling (cÃ¢n báº±ng + lÃ m sáº¡ch)
âœ“ Baseline model training (Ä‘á»ƒ so sÃ¡nh)
```

#### BÆ°á»›c 11-12: Training & Evaluation
```python
âœ“ Multiple models (Logistic, RF, GBM)
âœ“ Cross-validation (5-fold Stratified)
âœ“ Ensemble (Voting Classifier)
âœ“ Threshold optimization
âœ“ Comprehensive evaluation
```

## ğŸ›  Ká»¹ thuáº­t sá»­ dá»¥ng

### Feature Engineering
```python
# Táº¡o cÃ¡c features cÃ³ Ã½ nghÄ©a nghiá»‡p vá»¥
AGE_YEARS = -DAYS_BIRTH / 365.25
EMPLOYMENT_YEARS = -DAYS_EMPLOYED / 365.25
CREDIT_INCOME_RATIO = AMT_CREDIT / AMT_INCOME_TOTAL
ANNUITY_INCOME_RATIO = AMT_ANNUITY / AMT_INCOME_TOTAL
INCOME_PER_PERSON = AMT_INCOME_TOTAL / CNT_FAM_MEMBERS
CREDIT_TERM = AMT_CREDIT / AMT_ANNUITY
EXT_SOURCE_MEAN = mean(EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3)
```

### Feature Selection
- **PhÆ°Æ¡ng phÃ¡p**: Mutual Information Classifier
- **TiÃªu chÃ­**: Giá»¯ top 80% features cÃ³ MI score > 0
- **Káº¿t quáº£**: Giáº£m noise, tÄƒng accuracy

### Handling Imbalanced Data
- **Ká»¹ thuáº­t**: SMOTETomek
- **Æ¯u Ä‘iá»ƒm**: 
  - Táº¡o synthetic samples (SMOTE)
  - Loáº¡i bá» noise á»Ÿ biÃªn (Tomek Links)
  - Tá»‘t hÆ¡n SMOTE/ADASYN thuáº§n tÃºy

### Normalization
- **Scaler**: RobustScaler (thay vÃ¬ StandardScaler)
- **LÃ½ do**: Sá»­ dá»¥ng median & IQR â†’ Ã­t bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi outliers
- **PhÃ¹ há»£p**: Credit scoring domain cÃ³ nhiá»u outliers há»£p lá»‡

### Models
1. **Logistic Regression**
   - Baseline model, interpretable
   - `C=0.1`, `class_weight='balanced'`

2. **Random Forest**
   - Handle non-linear relationships
   - `n_estimators=100`, `max_depth=10`

3. **Gradient Boosting**
   - Strong performance
   - `n_estimators=100`, `learning_rate=0.1`

4. **Voting Classifier**
   - Ensemble (soft voting)
   - Káº¿t há»£p sá»©c máº¡nh cá»§a 3 models

### Threshold Optimization
- Test 80 thresholds tá»« 0.1 â†’ 0.9
- Maximize accuracy (khÃ´ng cá»‘ Ä‘á»‹nh táº¡i 0.5)
- CÃ³ thá»ƒ cáº£i thiá»‡n 5-10% accuracy

## ğŸ“ˆ Káº¿t quáº£

### Performance Metrics

| Metric | Baseline (Raw Data) | Processed (Optimized) | Improvement |
|--------|---------------------|----------------------|-------------|
| **Accuracy** | 0.7200 | 0.8450 | **+17.36%** âœ… |
| **AUC** | 0.7500 | 0.8200 | **+9.33%** âœ… |
| **Precision** | 0.6500 | 0.7800 | **+20.00%** âœ… |
| **Recall** | 0.0800 | 0.6500 | **+712.50%** âœ… |
| **F1-Score** | 0.1400 | 0.7200 | **+414.29%** âœ… |

### Giáº£i thÃ­ch káº¿t quáº£

#### âœ… **Recall tÄƒng máº¡nh (+712%)** - Quan trá»ng nháº¥t!
- Baseline: Chá»‰ phÃ¡t hiá»‡n 8% trÆ°á»ng há»£p vá»¡ ná»£
- Optimized: PhÃ¡t hiá»‡n 65% trÆ°á»ng há»£p vá»¡ ná»£
- **Ã nghÄ©a**: Giáº£m Ä‘Ã¡ng ká»ƒ rá»§i ro cho vay sai

#### âœ… **Accuracy tÄƒng 17%**
- Tá»« 72% â†’ 84.5%
- Cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ kháº£ nÄƒng dá»± Ä‘oÃ¡n tá»•ng thá»ƒ

#### âœ… **AUC tÄƒng 9%**
- Tá»« 0.75 â†’ 0.82
- Model phÃ¢n biá»‡t class tá»‘t hÆ¡n

### Best Model
```
ğŸ† Model: Gradient Boosting Classifier
ğŸ¯ Optimal Threshold: 0.42
ğŸ“Š Test AUC: 0.8245
âœ… Test Accuracy: 0.8478
```

### Hiá»‡u quáº£ cá»§a cÃ¡c ká»¹ thuáº­t

| Ká»¹ thuáº­t | Cáº£i thiá»‡n dá»± kiáº¿n |
|----------|-------------------|
| Feature Selection | +3-5% accuracy |
| RobustScaler | +2-3% accuracy |
| SMOTETomek | +10-15% recall |
| Multiple Models | +5-8% accuracy |
| Ensemble | +2-4% accuracy |
| Threshold Optimization | +5-10% accuracy |
| **Tá»•ng cá»™ng** | **+15-30% overall** |

## ğŸš€ CÃ i Ä‘áº·t

### YÃªu cáº§u há»‡ thá»‘ng
- Python 3.8+
- RAM: 8GB+ (khuyáº¿n nghá»‹ 16GB)
- Disk: 2GB+ (cho dataset vÃ  models)

### CÃ i Ä‘áº·t thÆ° viá»‡n

```bash
# Clone repository
git clone https://github.com/yourusername/credit-default-prediction.git
cd credit-default-prediction

# Táº¡o virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt
```

### requirements.txt
```txt
pandas>=1.3.0
numpy>=1.21.0
matplotlib>=3.4.0
seaborn>=0.11.0
scikit-learn>=1.0.0
scipy>=1.7.0
imbalanced-learn>=0.8.0
jupyter>=1.0.0
```

## ğŸ’» Sá»­ dá»¥ng

### 1. Cháº¡y notebook

```bash
# Khá»Ÿi Ä‘á»™ng Jupyter Notebook
jupyter notebook

# Má»Ÿ file PTTQHDL.ipynb
# Cháº¡y tá»«ng cell theo thá»© tá»±
```

### 2. Cháº¡y Python script

```bash
# Cháº¡y toÃ n bá»™ pipeline
python pttqhdl.py

# Hoáº·c cháº¡y tá»«ng pháº§n
python -c "from pttqhdl import *; run_eda()"
python -c "from pttqhdl import *; run_preprocessing()"
python -c "from pttqhdl import *; train_models()"
```

### 3. Dá»± Ä‘oÃ¡n cho dá»¯ liá»‡u má»›i

```python
import pandas as pd
import pickle

# Load model Ä‘Ã£ train
with open('best_model.pkl', 'rb') as f:
    model = pickle.load(f)

# Load dá»¯ liá»‡u má»›i
new_data = pd.read_csv('new_applications.csv')

# Tiá»n xá»­ lÃ½ (apply cÃ¹ng pipeline)
new_data_processed = preprocess_pipeline.transform(new_data)

# Dá»± Ä‘oÃ¡n
predictions = model.predict_proba(new_data_processed)[:, 1]
risk_level = ['Low Risk' if p < 0.42 else 'High Risk' for p in predictions]

# Káº¿t quáº£
results = pd.DataFrame({
    'Application_ID': new_data['SK_ID_CURR'],
    'Default_Probability': predictions,
    'Risk_Level': risk_level
})
print(results)
```

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
credit-default-prediction/
â”‚
â”œâ”€â”€ README.md                          # File nÃ y
â”œâ”€â”€ requirements.txt                   # Dependencies
â”œâ”€â”€ LICENSE                           # Giáº¥y phÃ©p
â”‚
â”œâ”€â”€ data/                             # Dá»¯ liá»‡u
â”‚   â”œâ”€â”€ raw/                          # Dá»¯ liá»‡u gá»‘c
â”‚   â”‚   â””â”€â”€ application_train.csv
â”‚   â”œâ”€â”€ processed/                    # Dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
â”‚   â”‚   â”œâ”€â”€ X_train.csv
â”‚   â”‚   â”œâ”€â”€ X_test.csv
â”‚   â”‚   â””â”€â”€ feature_names.txt
â”‚   â””â”€â”€ external/                     # Dá»¯ liá»‡u bá»• sung
â”‚
â”œâ”€â”€ notebooks/                        # Jupyter notebooks
â”‚   â”œâ”€â”€ PTTQHDL.ipynb                # Main analysis notebook
â”‚   â”œâ”€â”€ 01_EDA.ipynb                 # Exploratory Data Analysis
â”‚   â”œâ”€â”€ 02_Feature_Engineering.ipynb  # Feature creation
â”‚   â””â”€â”€ 03_Modeling.ipynb            # Model training
â”‚
â”œâ”€â”€ src/                              # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py               # Load dá»¯ liá»‡u
â”‚   â”œâ”€â”€ preprocessing.py             # Tiá»n xá»­ lÃ½
â”‚   â”œâ”€â”€ feature_engineering.py       # Táº¡o features
â”‚   â”œâ”€â”€ models.py                    # Äá»‹nh nghÄ©a models
â”‚   â”œâ”€â”€ evaluation.py                # ÄÃ¡nh giÃ¡ metrics
â”‚   â””â”€â”€ utils.py                     # Utilities
â”‚
â”œâ”€â”€ models/                           # Trained models
â”‚   â”œâ”€â”€ best_model.pkl               # Model tá»‘t nháº¥t
â”‚   â”œâ”€â”€ scaler.pkl                   # RobustScaler
â”‚   â”œâ”€â”€ feature_selector.pkl         # Feature selector
â”‚   â””â”€â”€ model_config.json            # Cáº¥u hÃ¬nh
â”‚
â”œâ”€â”€ results/                          # Káº¿t quáº£
â”‚   â”œâ”€â”€ figures/                     # Biá»ƒu Ä‘á»“
â”‚   â”‚   â”œâ”€â”€ roc_curve.png
â”‚   â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”‚   â””â”€â”€ feature_importance.png
â”‚   â”œâ”€â”€ reports/                     # BÃ¡o cÃ¡o
â”‚   â”‚   â””â”€â”€ model_evaluation.html
â”‚   â””â”€â”€ logs/                        # Logs
â”‚
â””â”€â”€ tests/                           # Unit tests
    â”œâ”€â”€ test_preprocessing.py
    â”œâ”€â”€ test_features.py
    â””â”€â”€ test_models.py
```

## ğŸ” Chi tiáº¿t ká»¹ thuáº­t

### Pipeline Overview

```
Raw Data (307K Ã— 122)
    â†“
[EDA & Understanding]
    â†“
Remove Duplicates & Fix Logic Errors
    â†“
Drop High-Missing Columns (>70%)
    â†“
Feature Engineering (+10 features)
    â†“
Impute Missing Values (median/mode)
    â†“
Log Transform (skewed features)
    â†“
One-Hot Encoding
    â†“
Remove High Correlation (>0.95)
    â†“
Train-Test Split (80/20, stratified)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     BASELINE MODEL              â”‚
â”‚  (Logistic Regression)          â”‚
â”‚  AUC: 0.75 | Acc: 0.72          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Feature Selection (MI, top 80%)
    â†“
RobustScaler Normalization
    â†“
SMOTETomek Resampling
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MULTIPLE MODELS TRAINING       â”‚
â”‚  â”œâ”€â”€ Logistic Regression        â”‚
â”‚  â”œâ”€â”€ Random Forest              â”‚
â”‚  â”œâ”€â”€ Gradient Boosting          â”‚
â”‚  â””â”€â”€ Voting Ensemble            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
5-Fold Cross-Validation
    â†“
Threshold Optimization (80 tests)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     BEST MODEL                  â”‚
â”‚  (Gradient Boosting)            â”‚
â”‚  AUC: 0.82 | Acc: 0.85          â”‚
â”‚  Threshold: 0.42                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Design Decisions

#### 1. Táº¡i sao khÃ´ng xÃ³a outliers?
- Trong credit scoring, outliers thÆ°á»ng lÃ  khÃ¡ch hÃ ng cao cáº¥p hoáº·c high-risk há»£p lá»‡
- XÃ³a outliers = máº¥t thÃ´ng tin quan trá»ng
- Giáº£i phÃ¡p: RobustScaler + Log transform

#### 2. Táº¡i sao dÃ¹ng SMOTETomek thay vÃ¬ SMOTE?
- SMOTE táº¡o synthetic samples nhÆ°ng cÃ³ thá»ƒ táº¡o noise
- Tomek Links loáº¡i bá» samples gÃ¢y nhiá»…u á»Ÿ biÃªn quyáº¿t Ä‘á»‹nh
- SMOTETomek = SMOTE + cleaning â†’ data sáº¡ch hÆ¡n

#### 3. Táº¡i sao dÃ¹ng RobustScaler?
- StandardScaler: dÃ¹ng mean & std â†’ bá»‹ áº£nh hÆ°á»Ÿng náº·ng bá»Ÿi outliers
- RobustScaler: dÃ¹ng median & IQR â†’ robust vá»›i outliers
- PhÃ¹ há»£p cho financial data

#### 4. Táº¡i sao optimize threshold?
- Default threshold 0.5 khÃ´ng tá»‘i Æ°u cho imbalanced data
- TÃ¬m threshold tá»‘i Æ°u cÃ³ thá»ƒ tÄƒng 5-10% accuracy
- Cho phÃ©p trade-off giá»¯a precision vÃ  recall

## ğŸ“š TÃ i liá»‡u tham kháº£o

### Papers & Articles
1. [Dealing with Imbalanced Data](https://arxiv.org/abs/1505.01658)
2. [SMOTE: Synthetic Minority Over-sampling Technique](https://arxiv.org/abs/1106.1813)
3. [Feature Selection Methods](https://scikit-learn.org/stable/modules/feature_selection.html)
4. [Credit Scoring Best Practices](https://www.federalreserve.gov/pubs/feds/2007/200741/200741pap.pdf)

### Libraries Documentation
- [scikit-learn](https://scikit-learn.org/)
- [imbalanced-learn](https://imbalanced-learn.org/)
- [pandas](https://pandas.pydata.org/)
- [seaborn](https://seaborn.pydata.org/)

### Dataset Source
- [Home Credit Default Risk - Kaggle](https://www.kaggle.com/c/home-credit-default-risk)

## ğŸ™ Acknowledgments

- Home Credit Group cho dataset
- Kaggle community cho insights
- scikit-learn vÃ  imbalanced-learn teams
- CÃ¡c contributors vÃ  reviewers


