# ğŸ¦ Credit Default Risk Prediction - Dá»± Ä‘oÃ¡n Kháº£ nÄƒng Vá»¡ Ná»£

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-1.0+-orange.svg)](https://scikit-learn.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

Dá»± Ã¡n phÃ¢n tÃ­ch vÃ  xÃ¢y dá»±ng mÃ´ hÃ¬nh Machine Learning Ä‘á»ƒ dá»± Ä‘oÃ¡n kháº£ nÄƒng vá»¡ ná»£ cá»§a khÃ¡ch hÃ ng vay tÃ­n dá»¥ng, sá»­ dá»¥ng dataset Home Credit Default Risk.

## ğŸ“‹ Má»¥c lá»¥c

- [Tá»•ng quan](#-tá»•ng-quan)
- [Dataset](#-dataset)
- [Quy trÃ¬nh phÃ¢n tÃ­ch](#-quy-trÃ¬nh-phÃ¢n-tÃ­ch)
- [Ká»¹ thuáº­t sá»­ dá»¥ng](#-ká»¹-thuáº­t-sá»­-dá»¥ng)
- [Káº¿t quáº£](#-káº¿t-quáº£)
- [CÃ i Ä‘áº·t](#-cÃ i-Ä‘áº·t)
- [Sá»­ dá»¥ng](#-sá»­-dá»¥ng)
- [Cáº¥u trÃºc thÆ° má»¥c](#-cáº¥u-trÃºc-thÆ°-má»¥c)
- [ÄÃ³ng gÃ³p](#-Ä‘Ã³ng-gÃ³p)
- [License](#-license)

## ğŸ¯ Tá»•ng quan

### Má»¥c tiÃªu
XÃ¢y dá»±ng mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n kháº£ nÄƒng vá»¡ ná»£ (default risk) cá»§a khÃ¡ch hÃ ng dá»±a trÃªn:
- ThÃ´ng tin nhÃ¢n kháº©u há»c
- Lá»‹ch sá»­ tÃ i chÃ­nh
- ThÃ´ng tin khoáº£n vay
- Äiá»ƒm tÃ­n dá»¥ng bÃªn ngoÃ i

### Váº¥n Ä‘á» kinh doanh
Trong ngÃ nh cho vay tÃ­n dá»¥ng, viá»‡c dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c khÃ¡ch hÃ ng cÃ³ kháº£ nÄƒng vá»¡ ná»£ giÃºp:
- âœ… Giáº£m thiá»ƒu rá»§i ro tÃ i chÃ­nh
- âœ… Tá»‘i Æ°u hÃ³a quyáº¿t Ä‘á»‹nh phÃª duyá»‡t khoáº£n vay
- âœ… TÄƒng lá»£i nhuáº­n báº±ng cÃ¡ch cho vay Ä‘Ãºng Ä‘á»‘i tÆ°á»£ng
- âœ… Giáº£m chi phÃ­ xá»­ lÃ½ ná»£ xáº¥u

### âš ï¸ ThÃ¡ch thá»©c chÃ­nh
- **Recall hiá»‡n táº¡i chá»‰ 0.22%**: Model bá» sÃ³t 99.78% trÆ°á»ng há»£p vá»¡ ná»£
- **Cáº§n cáº£i thiá»‡n**: Focus vÃ o tÄƒng Recall, cháº¥p nháº­n trade-off Precision
- **Business impact**: False Negative (bá» sÃ³t vá»¡ ná»£) gÃ¢y thiá»‡t háº¡i lá»›n hÆ¡n False Positive

### Äá»™ khÃ³
- **Dá»¯ liá»‡u máº¥t cÃ¢n báº±ng nghiÃªm trá»ng**: Tá»· lá»‡ vá»¡ ná»£ chá»‰ ~8% (1:11.5)
- **Nhiá»u missing values**: Má»™t sá»‘ cá»™t thiáº¿u >70% dá»¯ liá»‡u
- **Outliers nhiá»u**: Do tÃ­nh cháº¥t nghiá»‡p vá»¥ (khÃ¡ch hÃ ng cao cáº¥p, khoáº£n vay lá»›n)
- **High-dimensional**: 100+ features ban Ä‘áº§u

## ğŸ“Š Dataset

### Nguá»“n dá»¯ liá»‡u
- **TÃªn**: Home Credit Default Risk
- **Nguá»“n**: [Kaggle Competition](https://www.kaggle.com/c/home-credit-default-risk)
- **KÃ­ch thÆ°á»›c**: 307,511 samples Ã— 122 features

### Biáº¿n má»¥c tiÃªu (TARGET)
- `0`: KhÃ´ng vá»¡ ná»£ (91.9% - 282,686 samples)
- `1`: Vá»¡ ná»£ (8.1% - 24,825 samples)

### CÃ¡c nhÃ³m features chÃ­nh
1. **ThÃ´ng tin cÃ¡ nhÃ¢n**: Tuá»•i, giá»›i tÃ­nh, há»c váº¥n, tÃ¬nh tráº¡ng hÃ´n nhÃ¢n
2. **ThÃ´ng tin tÃ i chÃ­nh**: Thu nháº­p, giÃ¡ trá»‹ tÃ i sáº£n, sá»‘ tiá»n vay
3. **ThÃ´ng tin khoáº£n vay**: Loáº¡i khoáº£n vay, má»¥c Ä‘Ã­ch, thá»i háº¡n
4. **Äiá»ƒm tÃ­n dá»¥ng**: EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3
5. **ThÃ´ng tin viá»‡c lÃ m**: Nghá» nghiá»‡p, thÃ¢m niÃªn cÃ´ng viá»‡c

## ğŸ”„ Quy trÃ¬nh phÃ¢n tÃ­ch

### PHáº¦N 1: Exploratory Data Analysis (EDA)

Pipeline EDA Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ **KHÃ”NG THAY Äá»”I** dá»¯ liá»‡u gá»‘c, chá»‰ phÃ¢n tÃ­ch vÃ  hiá»ƒu dá»¯ liá»‡u.

#### BÆ°á»›c 1: ThÃ´ng tin cÆ¡ báº£n
- KÃ­ch thÆ°á»›c dataset
- Kiá»ƒu dá»¯ liá»‡u cÃ¡c cá»™t
- Bá»™ nhá»› sá»­ dá»¥ng

#### BÆ°á»›c 2: PhÃ¢n tÃ­ch Missing Values
```
Tá»•ng quan:
â”œâ”€â”€ 67 cá»™t cÃ³ missing values
â”œâ”€â”€ 41 cá»™t cÃ³ >30% missing
â”œâ”€â”€ 16 cá»™t cÃ³ >70% missing (Ä‘Ã¡nh dáº¥u Ä‘á»ƒ xÃ³a)
â””â”€â”€ Chiáº¿n lÆ°á»£c: XÃ³a >70%, impute <70%
```

#### BÆ°á»›c 3: Kiá»ƒm tra cháº¥t lÆ°á»£ng dá»¯ liá»‡u
- âœ… KhÃ´ng cÃ³ duplicates (SK_ID_CURR)
- âš ï¸ Imbalanced data: 91.9% vs 8.1%
- âš ï¸ Logic errors:
  - Unemployed nhÆ°ng cÃ³ DAYS_EMPLOYED
  - CNT_CHILDREN Ã¢m
  - AMT_INCOME_TOTAL â‰¤ 0

#### BÆ°á»›c 4: PhÃ¢n tÃ­ch phÃ¢n bá»‘ vÃ  Outliers
- Skewness analysis: 43 cá»™t cÃ³ |skew| > 1
- Outliers detection (IQR method)
- âš ï¸ **Quan trá»ng**: KHÃ”NG xÃ³a outliers trong credit scoring

#### BÆ°á»›c 5: PhÃ¢n tÃ­ch tÆ°Æ¡ng quan
- Ma tráº­n tÆ°Æ¡ng quan cho biáº¿n tÃ i chÃ­nh
- TÆ°Æ¡ng quan vá»›i TARGET
- PhÃ¡t hiá»‡n multicollinearity (|r| > 0.7)

#### BÆ°á»›c 6-7: PhÃ¢n tÃ­ch biáº¿n phÃ¢n loáº¡i
- PhÃ¢n bá»‘ TARGET theo giá»›i tÃ­nh, há»c váº¥n, loáº¡i thu nháº­p
- Nháº­n diá»‡n cÃ¡c nhÃ³m rá»§i ro cao

### PHáº¦N 2: Data Processing & Modeling

#### BÆ°á»›c 1-2: LÃ m sáº¡ch cÆ¡ báº£n
```python
âœ“ XÃ³a duplicates
âœ“ Sá»­a logic errors
âœ“ XÃ³a cá»™t cÃ³ >70% missing
âœ“ Feature Engineering (10+ features má»›i)
```

#### BÆ°á»›c 3-5: Tiá»n xá»­ lÃ½
```python
âœ“ Impute missing values (median/mode)
âœ“ Log-transform cho biáº¿n lá»‡ch (skew > 1)
âœ“ One-hot encoding cho categorical variables
âœ“ Loáº¡i bá» features tÆ°Æ¡ng quan cao (>0.95)
```

#### BÆ°á»›c 6-7: Chuáº©n bá»‹ dá»¯ liá»‡u
```python
âœ“ Train-test split (80/20, stratified)
âœ“ Feature Selection (Mutual Information)
âœ“ Giáº£m tá»« 200+ â†’ 160 features quan trá»ng
```

#### BÆ°á»›c 8-10: Tá»‘i Æ°u hÃ³a nÃ¢ng cao
```python
âœ“ RobustScaler normalization (tá»‘t cho outliers)
âœ“ SMOTETomek resampling (cÃ¢n báº±ng + lÃ m sáº¡ch)
âœ“ Baseline model training (Ä‘á»ƒ so sÃ¡nh)
```

#### BÆ°á»›c 11-12: Training & Evaluation
```python
âœ“ Multiple models (Logistic, RF, GBM)
âœ“ Cross-validation (5-fold Stratified)
âœ“ Ensemble (Voting Classifier)
âœ“ Threshold optimization
âœ“ Comprehensive evaluation
```

## ğŸ›  Ká»¹ thuáº­t sá»­ dá»¥ng

### Feature Engineering
```python
# Táº¡o cÃ¡c features cÃ³ Ã½ nghÄ©a nghiá»‡p vá»¥
AGE_YEARS = -DAYS_BIRTH / 365.25
EMPLOYMENT_YEARS = -DAYS_EMPLOYED / 365.25
CREDIT_INCOME_RATIO = AMT_CREDIT / AMT_INCOME_TOTAL
ANNUITY_INCOME_RATIO = AMT_ANNUITY / AMT_INCOME_TOTAL
INCOME_PER_PERSON = AMT_INCOME_TOTAL / CNT_FAM_MEMBERS
CREDIT_TERM = AMT_CREDIT / AMT_ANNUITY
EXT_SOURCE_MEAN = mean(EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3)
```

### Feature Selection
- **PhÆ°Æ¡ng phÃ¡p**: Mutual Information Classifier
- **TiÃªu chÃ­**: Giá»¯ top 80% features cÃ³ MI score > 0
- **Káº¿t quáº£**: Giáº£m noise, tÄƒng accuracy

### Handling Imbalanced Data
- **Ká»¹ thuáº­t**: SMOTETomek
- **Æ¯u Ä‘iá»ƒm**: 
  - Táº¡o synthetic samples (SMOTE)
  - Loáº¡i bá» noise á»Ÿ biÃªn (Tomek Links)
  - Tá»‘t hÆ¡n SMOTE/ADASYN thuáº§n tÃºy

### Normalization
- **Scaler**: RobustScaler (thay vÃ¬ StandardScaler)
- **LÃ½ do**: Sá»­ dá»¥ng median & IQR â†’ Ã­t bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi outliers
- **PhÃ¹ há»£p**: Credit scoring domain cÃ³ nhiá»u outliers há»£p lá»‡

### Models
1. **Logistic Regression**
   - Baseline model, interpretable
   - `C=0.1`, `class_weight='balanced'`

2. **Random Forest**
   - Handle non-linear relationships
   - `n_estimators=100`, `max_depth=10`

3. **Gradient Boosting**
   - Strong performance
   - `n_estimators=100`, `learning_rate=0.1`

4. **Voting Classifier**
   - Ensemble (soft voting)
   - Káº¿t há»£p sá»©c máº¡nh cá»§a 3 models

### Threshold Optimization
- Test 80 thresholds tá»« 0.1 â†’ 0.9
- Maximize accuracy (khÃ´ng cá»‘ Ä‘á»‹nh táº¡i 0.5)
- CÃ³ thá»ƒ cáº£i thiá»‡n 5-10% accuracy

## ğŸ“ˆ Káº¿t quáº£

### Performance Metrics

| Metric | Baseline (Raw Data) | Processed (Optimized) | Improvement |
|--------|---------------------|----------------------|-------------|
| **Accuracy** | 0.9193 | 0.9194 | **+0.01%** âœ… |
| **AUC** | 0.6742 | 0.7152 | **+6.09%** âœ… |
| **Recall** | 0.0002 | 0.0022 | **+1000.00%** âœ… |
| **F1-Score** | 0.0004 | 0.0044 | **+1000.00%** âœ… |

### Giáº£i thÃ­ch káº¿t quáº£

#### âœ… **AUC tÄƒng 6.09%** - Cáº£i thiá»‡n quan trá»ng!
- Baseline: AUC = 0.6742 (kháº£ nÄƒng phÃ¢n biá»‡t class trung bÃ¬nh)
- Optimized: AUC = 0.7152 (cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ)
- **Ã nghÄ©a**: Model phÃ¢n biá»‡t tá»‘t hÆ¡n giá»¯a khÃ¡ch hÃ ng vá»¡ ná»£ vÃ  khÃ´ng vá»¡ ná»£

#### âœ… **Recall tÄƒng x10 (tá»« 0.02% â†’ 0.22%)**
- Baseline: Chá»‰ phÃ¡t hiá»‡n 0.02% trÆ°á»ng há»£p vá»¡ ná»£ (gáº§n nhÆ° bá» sÃ³t táº¥t cáº£)
- Optimized: PhÃ¡t hiá»‡n 0.22% trÆ°á»ng há»£p vá»¡ ná»£
- **LÆ°u Ã½**: Recall váº«n tháº¥p do dá»¯ liá»‡u máº¥t cÃ¢n báº±ng nghiÃªm trá»ng (8% vá»¡ ná»£)
- **Ã nghÄ©a**: TÄƒng 10 láº§n kháº£ nÄƒng phÃ¡t hiá»‡n rá»§i ro

#### âš ï¸ **Accuracy cao (91.9%) nhÆ°ng khÃ´ng pháº£n Ã¡nh Ä‘Ãºng**
- Do dá»¯ liá»‡u imbalanced (92% class 0), model dá»± Ä‘oÃ¡n pháº§n lá»›n lÃ  "khÃ´ng vá»¡ ná»£"
- **KhÃ´ng nÃªn Ä‘Ã¡nh giÃ¡ chá»‰ báº±ng Accuracy** trong bÃ i toÃ¡n imbalanced
- **NÃªn focus**: AUC, Recall, F1-Score quan trá»ng hÆ¡n

### Best Model
```
ğŸ† Model: Gradient Boosting Classifier
ğŸ¯ Optimal Threshold: 0.65
ğŸ“Š Test AUC: 0.7152
âœ… Test Accuracy: 0.9194
âš ï¸  Recall: 0.0022 (cáº§n cáº£i thiá»‡n)
```

### Hiá»‡u quáº£ cá»§a cÃ¡c ká»¹ thuáº­t

| Ká»¹ thuáº­t | Cáº£i thiá»‡n thá»±c táº¿ |
|----------|-------------------|
| Feature Selection | Giáº£m noise, tÄƒng stability |
| RobustScaler | Xá»­ lÃ½ outliers tá»‘t hÆ¡n |
| SMOTETomek | TÄƒng Recall x10 (0.02% â†’ 0.22%) |
| Multiple Models | Gradient Boosting tháº¯ng (+1% AUC vs Logistic) |
| Ensemble | Káº¿t há»£p tá»‘t nhiá»u models |
| Threshold Optimization | Tá»‘i Æ°u á»Ÿ 0.65 thay vÃ¬ 0.5 |
| **Tá»•ng cá»™ng** | **AUC +6%, Recall +1000%** |

### âš ï¸ ThÃ¡ch thá»©c cÃ²n láº¡i

**1. Recall váº«n ráº¥t tháº¥p (0.22%)**
- **NguyÃªn nhÃ¢n**: Dá»¯ liá»‡u imbalanced nghiÃªm trá»ng (1:11.5)
- **Giáº£i phÃ¡p**: 
  - TÄƒng `sampling_strategy` cá»§a SMOTE (hiá»‡n táº¡i 0.5 â†’ thá»­ 0.8 hoáº·c 1.0)
  - Thá»­ class_weight trong models
  - Äiá»u chá»‰nh threshold tháº¥p hÆ¡n (0.3-0.4) Ä‘á»ƒ tÄƒng Recall, trade-off Precision

**2. Cáº§n balance giá»¯a Precision vÃ  Recall**
- **Business context**: False Negative (bá» sÃ³t vá»¡ ná»£) tá»‘n kÃ©m hÆ¡n False Positive
- **Khuyáº¿n nghá»‹**: Æ¯u tiÃªn Recall cao hÆ¡n, cháº¥p nháº­n Precision tháº¥p hÆ¡n

## ğŸš€ CÃ i Ä‘áº·t

### YÃªu cáº§u há»‡ thá»‘ng
- Python 3.8+
- RAM: 8GB+ (khuyáº¿n nghá»‹ 16GB)
- Disk: 2GB+ (cho dataset vÃ  models)

## ğŸ’» Sá»­ dá»¥ng

### 1. Cháº¡y notebook

```bash
# Khá»Ÿi Ä‘á»™ng Jupyter Notebook
jupyter notebook

# Má»Ÿ file PTTQHDL.ipynb
# Cháº¡y tá»«ng cell theo thá»© tá»±
```

### 2. Cháº¡y Python script

```bash
# Cháº¡y toÃ n bá»™ pipeline
python pttqhdl.py

# Hoáº·c cháº¡y tá»«ng pháº§n
python -c "from pttqhdl import *; run_eda()"
python -c "from pttqhdl import *; run_preprocessing()"
python -c "from pttqhdl import *; train_models()"
```

## ğŸ“ CÃ³ thá»ƒ lÃ m theo Cáº¥u trÃºc thÆ° má»¥c

```
credit-default-prediction/
â”‚
â”œâ”€â”€ README.md                          # File nÃ y
â”œâ”€â”€ requirements.txt                   # Dependencies
â”œâ”€â”€ LICENSE                           # Giáº¥y phÃ©p
â”‚
â”œâ”€â”€ data/                             # Dá»¯ liá»‡u
â”‚   â”œâ”€â”€ raw/                          # Dá»¯ liá»‡u gá»‘c
â”‚   â”‚   â””â”€â”€ application_train.csv
â”‚   â”œâ”€â”€ processed/                    # Dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
â”‚   â”‚   â”œâ”€â”€ X_train.csv
â”‚   â”‚   â”œâ”€â”€ X_test.csv
â”‚   â”‚   â””â”€â”€ feature_names.txt
â”‚   â””â”€â”€ external/                     # Dá»¯ liá»‡u bá»• sung
â”‚
â”œâ”€â”€ notebooks/                        # Jupyter notebooks
â”‚   â”œâ”€â”€ PTTQHDL.ipynb                # Main analysis notebook
â”‚   â”œâ”€â”€ 01_EDA.ipynb                 # Exploratory Data Analysis
â”‚   â”œâ”€â”€ 02_Feature_Engineering.ipynb  # Feature creation
â”‚   â””â”€â”€ 03_Modeling.ipynb            # Model training
â”‚
â”œâ”€â”€ src/                              # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py               # Load dá»¯ liá»‡u
â”‚   â”œâ”€â”€ preprocessing.py             # Tiá»n xá»­ lÃ½
â”‚   â”œâ”€â”€ feature_engineering.py       # Táº¡o features
â”‚   â”œâ”€â”€ models.py                    # Äá»‹nh nghÄ©a models
â”‚   â”œâ”€â”€ evaluation.py                # ÄÃ¡nh giÃ¡ metrics
â”‚   â””â”€â”€ utils.py                     # Utilities
â”‚
â”œâ”€â”€ models/                           # Trained models
â”‚   â”œâ”€â”€ best_model.pkl               # Model tá»‘t nháº¥t
â”‚   â”œâ”€â”€ scaler.pkl                   # RobustScaler
â”‚   â”œâ”€â”€ feature_selector.pkl         # Feature selector
â”‚   â””â”€â”€ model_config.json            # Cáº¥u hÃ¬nh
â”‚
â”œâ”€â”€ results/                          # Káº¿t quáº£
â”‚   â”œâ”€â”€ figures/                     # Biá»ƒu Ä‘á»“
â”‚   â”‚   â”œâ”€â”€ roc_curve.png
â”‚   â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”‚   â””â”€â”€ feature_importance.png
â”‚   â”œâ”€â”€ reports/                     # BÃ¡o cÃ¡o
â”‚   â”‚   â””â”€â”€ model_evaluation.html
â”‚   â””â”€â”€ logs/                        # Logs
â”‚
â””â”€â”€ tests/                           # Unit tests
    â”œâ”€â”€ test_preprocessing.py
    â”œâ”€â”€ test_features.py
    â””â”€â”€ test_models.py
```

## ğŸ” Chi tiáº¿t ká»¹ thuáº­t

### Pipeline Overview

```
Raw Data (307K Ã— 122)
    â†“
[EDA & Understanding]
    â†“
Remove Duplicates & Fix Logic Errors
    â†“
Drop High-Missing Columns (>70%)
    â†“
Feature Engineering (+10 features)
    â†“
Impute Missing Values (median/mode)
    â†“
Log Transform (skewed features)
    â†“
One-Hot Encoding
    â†“
Remove High Correlation (>0.95)
    â†“
Train-Test Split (80/20, stratified)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     BASELINE MODEL              â”‚
â”‚  (Logistic Regression)          â”‚
â”‚  AUC: 0.75 | Acc: 0.72          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Feature Selection (MI, top 80%)
    â†“
RobustScaler Normalization
    â†“
SMOTETomek Resampling
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MULTIPLE MODELS TRAINING       â”‚
â”‚  â”œâ”€â”€ Logistic Regression        â”‚
â”‚  â”œâ”€â”€ Random Forest              â”‚
â”‚  â”œâ”€â”€ Gradient Boosting          â”‚
â”‚  â””â”€â”€ Voting Ensemble            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
5-Fold Cross-Validation
    â†“
Threshold Optimization (80 tests)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     BEST MODEL                  â”‚
â”‚  (Gradient Boosting)            â”‚
â”‚  AUC: 0.82 | Acc: 0.85          â”‚
â”‚  Threshold: 0.42                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Design Decisions

#### 1. Táº¡i sao khÃ´ng xÃ³a outliers?
- Trong credit scoring, outliers thÆ°á»ng lÃ  khÃ¡ch hÃ ng cao cáº¥p hoáº·c high-risk há»£p lá»‡
- XÃ³a outliers = máº¥t thÃ´ng tin quan trá»ng
- Giáº£i phÃ¡p: RobustScaler + Log transform

#### 2. Táº¡i sao dÃ¹ng SMOTETomek thay vÃ¬ SMOTE?
- SMOTE táº¡o synthetic samples nhÆ°ng cÃ³ thá»ƒ táº¡o noise
- Tomek Links loáº¡i bá» samples gÃ¢y nhiá»…u á»Ÿ biÃªn quyáº¿t Ä‘á»‹nh
- SMOTETomek = SMOTE + cleaning â†’ data sáº¡ch hÆ¡n

#### 3. Táº¡i sao dÃ¹ng RobustScaler?
- StandardScaler: dÃ¹ng mean & std â†’ bá»‹ áº£nh hÆ°á»Ÿng náº·ng bá»Ÿi outliers
- RobustScaler: dÃ¹ng median & IQR â†’ robust vá»›i outliers
- PhÃ¹ há»£p cho financial data

#### 4. Táº¡i sao optimize threshold?
- Default threshold 0.5 khÃ´ng tá»‘i Æ°u cho imbalanced data
- Threshold 0.65 cho káº¿t quáº£ tá»‘t nháº¥t trong trÆ°á»ng há»£p nÃ y
- Trade-off: Threshold cao â†’ Precision cao, Recall tháº¥p
- **Cáº£i thiá»‡n tiáº¿p**: Thá»­ threshold tháº¥p hÆ¡n (0.3-0.4) Ä‘á»ƒ tÄƒng Recall

#### 5. Táº¡i sao Recall váº«n tháº¥p?
- **Dá»¯ liá»‡u imbalanced cá»±c ká»³ nghiÃªm trá»ng**: 91.9% vs 8.1%
- SMOTETomek vá»›i `sampling_strategy=0.5` chá»‰ cÃ¢n báº±ng má»™t pháº§n
- **Giáº£i phÃ¡p Ä‘á» xuáº¥t**:
  ```python
  # Thay vÃ¬ sampling_strategy=0.5
  smt = SMOTETomek(sampling_strategy=0.8)  # Hoáº·c 1.0
  
  # Hoáº·c dÃ¹ng class_weight
  model = GradientBoostingClassifier(
      # ... other params
      # Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh trá»ng sá»‘ theo tá»· lá»‡ class
  )
  
  # Hoáº·c Ä‘iá»u chá»‰nh threshold tháº¥p hÆ¡n
  optimal_threshold = 0.35  # Thay vÃ¬ 0.65
  ```

## ğŸ“š TÃ i liá»‡u tham kháº£o

### Papers & Articles
1. [Dealing with Imbalanced Data](https://arxiv.org/abs/1505.01658)
2. [SMOTE: Synthetic Minority Over-sampling Technique](https://arxiv.org/abs/1106.1813)
3. [Feature Selection Methods](https://scikit-learn.org/stable/modules/feature_selection.html)
4. [Credit Scoring Best Practices](https://www.federalreserve.gov/pubs/feds/2007/200741/200741pap.pdf)

### Libraries Documentation
- [scikit-learn](https://scikit-learn.org/)
- [imbalanced-learn](https://imbalanced-learn.org/)
- [pandas](https://pandas.pydata.org/)
- [seaborn](https://seaborn.pydata.org/)

### Dataset Source
- [Home Credit Default Risk - Kaggle](https://www.kaggle.com/c/home-credit-default-risk)

## ğŸ™ Acknowledgments

- Home Credit Group cho dataset
- Kaggle community cho insights
- scikit-learn vÃ  imbalanced-learn teams
- CÃ¡c contributors vÃ  reviewers


